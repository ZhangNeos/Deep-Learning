{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 深度前馈网络简介  \n",
    "\n",
    "   1.**深度前馈网络**(deep feedforward network)，也叫作前馈神经网络(feedforward neural net-work)或者多层感知机(multilayer perceptron, MLP)，是典型的深度学习模型\n",
    "    \n",
    "   2.这种模型被称为前向(feedforward)的，是因为**在模型的输出和模型本身之间没有反馈连接**。当前馈神经网络被扩展成包含反馈连接时，它们被称为循环神经网络(recurrent neural network)\n",
    "    \n",
    "   3.前馈神经网络之所以被称为网络(network)，是因为它们通常用许多不同的函数复合在一起来表示，该模型与一个有向无环图关联，而图描述了函数是如何复合在一起的\n",
    "    \n",
    "   4.训练数据为我们提供了在不同训练点上取值的、带有噪声的真实分布的近似实例，每个样本都伴随着一个标签。训练样本直接指明了输出层在每一点x上必须做什么：它必须产生一个接近y的值\n",
    "    \n",
    "   5.这些网络之所以被称为神经网络，是因多它们或多或少地受到神经科学的启发。网络中的每个隐藏层通常都是向量值的。这个隐藏层的维数决定了模型的宽度。向量的每个元素都可以被视为起到一个类似神经元的作用。除了将层想象成向量到向量的单个函数，我们也**可以把层想象成由许多并行操作的单元组成，每个单元表示一个向量到标量的函数**。每个单元在某种意义上类似一个神经元，它接收的输入来源于许多其他的单元，并计算它自己的激活值\n",
    " \n",
    " ### 1.1.1 整流线性函数\n",
    "   6.整流线性函数是被推荐用于大多数前馈神经网络的默认激活函数。将此函数用于线性变换的输出将产生非线性变换。然而，函数仍然非常接近线性，在这种意义上它是具有两个线性部分的分段线性函数\n",
    "   \n",
    "   7.由于整流线性单元几乎是线性的，因此它保留了许多使得线性模型易于使用基于梯度的方法进行优化的属性。它们还保留了许多使得线性模型能够泛化的良好属性\n",
    "   \n",
    "   8.**利用线性整流函数可以对输入的组合特征进行进一步的筛选**，类似图灵机的内存只需要能够存储0或1的状态，我们可以从整流线性函数构建一个万能函数近似器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 基于梯度的学习\n",
    "   1.设计和训练神经网络与使用梯度下降训练其他任何机器学习模型并没有太大的不同\n",
    "   \n",
    "   2.线性模型和神经网络的最大区别，在于**神经网络的非线性导致大多数我们感兴趣的代价函数都变得非凸**。这意味着神经网络的训练通常使用迭代的、基于梯度的优化，仅仅使得代价函数达到一个非常小的值；而不是像用于训练线性回归模型的线性方程求解器，或者用于训练逻辑回归或SVM的凸优化算法那样保证全局收敛\n",
    "   \n",
    "   3.凸优化从任何一种初始参数出发都会收敛。**用于非凸损失函数的随机梯度下降没有这种收敛性保证**，并且对参数的初始值很敏感。对于前馈神经网络，将所有权重初始化为小随机数是很重要的。偏执可以初始化为0或小的正值\n",
    "   \n",
    "### 1.2.1 代价函数\n",
    "   4.大多数情况下，参数模型定义了一个分布p(y|x;θ)并且简单地使用最大似然原理。这意味着我们**使用训练数据和模型预测间的交叉熵作为代价函数**\n",
    "   \n",
    "   5.有时，我们使用一个更简单的方法，不是预测y的完整概率分布，而是仅仅预测在给定x的条件下y的某种统计量\n",
    "   \n",
    "   6.用于训练神经网络的完整的代价函数，通常在基本代价函数的基础上结合一个正则项(用于线性模型的权重衰减方法也直接用于深度神经网络)\n",
    "   \n",
    "   7.**大多数现代的神经网络使用最大似然来训练，这意味着代价函数就是负的对数似然，它与训练数据和模型分布间的交叉熵等价**\n",
    "   \n",
    "   8.贯穿神经网络设计的一个反复出现的主题是代价函数的梯度必须必须足够大和足有足够的预测性，来为学习提供一个好的指引。饱和的函数破坏了这一目标，因为它们把梯度变得非常小。\n",
    "   \n",
    "   9.在很多情况下，很多输出单元都会包含一个指数函数，这在它的变量绝对值很大时会造成饱和。负的对数似然可以在很多模型中避免这个问题。**负对数似然代价函数消除了某些输出单元中的指数效果**\n",
    "   \n",
    "   10.均方误差和平均绝对误差在使用基于梯度的优化方法时往往成效不佳。一些饱和的输出单元当结合这些代价函数时会产生非常小的梯度\n",
    "   \n",
    "### 1.2.2 输出单元\n",
    "\n",
    "   1.代价函数的选择与输出单元的选择紧密相关。大多数时候，我们简单地使用数据分布和模型分布间的交叉熵。选择如何表示输出决定了交叉熵函数的形式\n",
    "\n",
    "#### 1.2.2.1 用于高斯输出分布的线性单元\n",
    "    * 一种简单的输出单元是基于仿射变换的输出单元，仿射变换不具有非线性。这些单元往往被直接称为线性单元\n",
    "    * 线性输出层常被用来产生条件高斯分布的均值，最大化其似然函数等价于最小化均方误差\n",
    "    * 因为线性单元不会饱和，所以它们易于采用基于梯度的优化算法\n",
    "#### 1.2.2.2 用于伯努利输出分布的sigmoid单元\n",
    "    * z = W*x+b\n",
    "    * y = σ(z)=W*v+b\n",
    "    * 许多任务需要预测二值型变量y的值，具有两个类的分类问题可以归结为这种形式\n",
    "    * 此时最大似然方法是定义y在x条件下的伯努利分布\n",
    "    * 伯努利分布仅需单个参数来定义。神经网络只需要预测P(y=1|x)即可。为了让这个数是有效的概率，它必须处在区间(0,1)中\n",
    "    * 我们希望找到一种方法来保证，无论何时模型给出错误答案时总能有一个较大的梯度\n",
    "    * 这种方法是基于使用sigmoid输出单元结合最大似然来实现的\n",
    "    * 我们可以认为sigmoid输出单元具有两个部分，首先，它使用一个线性层进行计算，其次，使用sigmoid激活函数将线性计算的结果转换成非归一化(和不为1)概率\n",
    "    * sigmoid可以通过先构造一个非归一化的概率分布来得到，随后除以一个合适的常数来得到有效的概率分布。\n",
    "    * 基于指数和归一化的概率分布在统计建模中很常用。用于定义这种二值型变量分布变量的变量被称为分对数(logit)\n",
    "    * 这种在对数空间里预测概率的方法可以很自然地使用最大似然学习。因为用于最大似然的代价函数是-logP(y|x),代价函数中的log抵消了sigmoid中的exp。如果使用其他损失函数，sigmoid的饱和性会阻止基于梯度学习做出好的改进\n",
    "    * 负的最大似然几乎总是训练sigmoid输出单元的优选方法\n",
    "#### 1.2.2.3 用于多项输出分布softmax单元\n",
    "    * 当想要表示一个具有n个可能取值的离散型随机变量的分布时，都可以使用softmax函数，它可以看作sigmoid函数的扩展，其中sigmoid函数用来表示二值型变量的分布\n",
    "    * softmax的名称可能会让人产生疑惑。这个函数更接近于argmax函数而不是max函数\n",
    "    * softmax函数是对输入之间差异的响应\n",
    "    * softmax函数最常用作分类器的输出，来表示n个不同类上的概率分布\n",
    "    * 将softmax定义成指数的形式是很自然的，因为对数似然中的log可以抵消softmax中的exp\n",
    "    * 与sigmoid类似，softmax函数也会饱和。对于softmax的情况，它有多个输出值，当输入值之间的差异非常极端时，这些输出值可能饱和。当softmax饱和时，基于softmax的许多代价函数也饱和，除非它们能够转化饱和的激活函数\n",
    "\n",
    "## 1.3 隐藏单元\n",
    "    * 隐藏单元的设计是一个非常活跃的研究领域，目前还没有许多明确的指导性原则\n",
    "    * 设计过程充满了试验和错误，先直觉认为某种隐藏单元可能表现良好，然后用它组成神经网络进行训练，最后用验证集来评估它的性能\n",
    "    * 整流线性单元是隐藏单元极好的默认选择\n",
    "### 1.3.1 整流线性单元及其扩展\n",
    "    * 整流线性单元易于优化，因为它们和线性单元非常类似。线性单元和整流线性单元的唯一区别在于整流线性单元在其一半的定义域上输出为0。这使得只要整流线性单元处于激活状态，它的倒数都能保持较大。它的梯度不但大而且一致。整流操作的二阶导数几乎处处为0，并且在整流线性单元处于激活状态时，它的一阶导数处处为1。相比于引入二阶效应的激活函数来说，它在梯度方向上对于学习来说更加有用。\n",
    "    * 当初始化仿射变换操作时，可以将偏执的所有元素设置成一个小的正值，例如0.1， 这使得整流线性单元很可能初始时就对训练集大多数输入呈现激活状态，并且允许导数通过\n",
    "    * 整流线性单元的一个缺陷是它们不能通过基于梯度的方法学习那些使它们激活为0的样本。整流线性单元的各种扩展(基于当z<0时使用一个非零的梯度)保证了它们能在各个位置都接收到梯度\n",
    "    \n",
    "    * maxout单元进一步扩展了整流线性单元。maxout单元将z划分为每组具有k个值的组，而不是使用作用于每个元素的函数g(z)。每个maxout单元则输出每组中的最大元素\n",
    "    * maxout单元可以学习多达k段的分段线性的凸函数。maxout单元因此可以视为学习激活函数本身，而不仅仅是单元之间的关系。使用足够大的k，maxout单元可以以任意的精确度来近似任何凸函数\n",
    "    * 每个maxout单元现在由k个权重向量来参数化，而不仅仅是一个，所以maxout单元通常比整流线性单元需要更多的正则化。如果训练集很大并且每个单元的块数保持很低的话，它们可以在没有正则化的情况下工作得不错\n",
    "    \n",
    "    * 整流线性单元和它们的这些扩展都是基于一个原则，那就是如果它们的行为更接近线性，那么模型更容易优化。使用线性行为更容易优化的一般性原则也适用于除深度线性网络以外的场景\n",
    "    \n",
    "### 1.3.2 logistic sigmoid与双曲正切函数\n",
    "    * 在引入整流线性单元之前，大多数神经网络使用logistic sigmoid激活函数或双曲正切激活函数\n",
    "    * 与分段线性单元不同，simoid单元在其大部分定义域内饱和——仅仅当z接近于0时才对输入强烈敏感\n",
    "    * sigmoiid单元的广泛饱和性会使得基于梯度的学习变得非常困难，因此，不鼓励将它们用作前馈网络中的隐藏单元\n",
    "    * 当必须使用sigmoid激活函数时，双曲正切激活函数通常要比logistic sigmoid函数表现得更好。在tanh(0)=0而σ(0)=1/2的意义上，它更像是单位函数。因为tanh在0附近与单位函数类似，只要网络的激活能够保持地很小训练tanh深层神经网络类似于训练一个线性模型。这使得训练tanh网络更加容易\n",
    "    \n",
    "### 1.3.3 其他隐藏单元\n",
    "\n",
    "## 1.4 架构设计\n",
    "    * 神经网络设计的另一个关键点是确定它的架构。架构是指网络的整体结构:它应该具有多少单元，以及这些单元应该改如何连接\n",
    "    * 大多数神经网络被组织成称为层的单元组。大多数神经网络架构将这些层布置成链式结构，其中每一层都是前一层的函数\n",
    "    * 在这些链式结构中，主要的架构考虑是选择网络的深度和宽度。即使只有一个隐藏层的网络也足够适应训练集。更深层的网络通常能够对每一层使用更少的单元数和更少的参数，并且经常容易泛化到测试集，但通常更加难以优化。\n",
    "    * 万能近似定理意味着无论学习什么函数，一个大的MLP一定能够表示这个函数。然而，即使MLP能够表示该函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
