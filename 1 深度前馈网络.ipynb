{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 深度前馈网络简介  \n",
    "\n",
    "   1.**深度前馈网络**(deep feedforward network)，也叫作前馈神经网络(feedforward neural net-work)或者多层感知机(multilayer perceptron, MLP)，是典型的深度学习模型\n",
    "    \n",
    "   2.这种模型被称为前向(feedforward)的，是因为**在模型的输出和模型本身之间没有反馈连接**。当前馈神经网络被扩展成包含反馈连接时，它们被称为循环神经网络(recurrent neural network)\n",
    "    \n",
    "   3.前馈神经网络之所以被称为网络(network)，是因为它们通常用许多不同的函数复合在一起来表示，该模型与一个有向无环图关联，而图描述了函数是如何复合在一起的\n",
    "    \n",
    "   4.训练数据为我们提供了在不同训练点上取值的、带有噪声的真实分布的近似实例，每个样本都伴随着一个标签。训练样本直接指明了输出层在每一点x上必须做什么：它必须产生一个接近y的值\n",
    "    \n",
    "   5.这些网络之所以被称为神经网络，是因多它们或多或少地受到神经科学的启发。网络中的每个隐藏层通常都是向量值的。这个隐藏层的维数决定了模型的宽度。向量的每个元素都可以被视为起到一个类似神经元的作用。除了将层想象成向量到向量的单个函数，我们也**可以把层想象成由许多并行操作的单元组成，每个单元表示一个向量到标量的函数**。每个单元在某种意义上类似一个神经元，它接收的输入来源于许多其他的单元，并计算它自己的激活值\n",
    " \n",
    " ### 1.1.1 整流线性函数\n",
    "   6.整流线性函数是被推荐用于大多数前馈神经网络的默认激活函数。将此函数用于线性变换的输出将产生非线性变换。然而，函数仍然非常接近线性，在这种意义上它是具有两个线性部分的分段线性函数\n",
    "   \n",
    "   7.由于整流线性单元几乎是线性的，因此它保留了许多使得线性模型易于使用基于梯度的方法进行优化的属性。它们还保留了许多使得线性模型能够泛化的良好属性\n",
    "   \n",
    "   8.**利用线性整流函数可以对输入的组合特征进行进一步的筛选**，类似图灵机的内存只需要能够存储0或1的状态，我们可以从整流线性函数构建一个万能函数近似器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 基于梯度的学习\n",
    "   1.设计和训练神经网络与使用梯度下降训练其他任何机器学习模型并没有太大的不同\n",
    "   \n",
    "   2.线性模型和神经网络的最大区别，在于**神经网络的非线性导致大多数我们感兴趣的代价函数都变得非凸**。这意味着神经网络的训练通常使用迭代的、基于梯度的优化，仅仅使得代价函数达到一个非常小的值；而不是像用于训练线性回归模型的线性方程求解器，或者用于训练逻辑回归或SVM的凸优化算法那样保证全局收敛\n",
    "   \n",
    "   3.凸优化从任何一种初始参数出发都会收敛。**用于非凸损失函数的随机梯度下降没有这种收敛性保证**，并且对参数的初始值很敏感。对于前馈神经网络，将所有权重初始化为小随机数是很重要的。偏执可以初始化为0或小的正值\n",
    "   \n",
    "### 6.2.1 代价函数\n",
    "   4.大多数情况下，参数模型定义了一个分布p(y|x;θ)并且简单地使用最大似然原理。这意味着我们**使用训练数据和模型预测间的交叉熵作为代价函数**\n",
    "   \n",
    "   5.有时，我们使用一个更简单的方法，不是预测y的完整概率分布，而是仅仅预测在给定x的条件下y的某种统计量\n",
    "   \n",
    "   6.用于训练神经网络的完整的代价函数，通常在基本代价函数的基础上结合一个正则项(用于线性模型的权重衰减方法也直接用于深度神经网络)\n",
    "   \n",
    "   7.**大多数现代的神经网络使用最大似然来训练，这意味着代价函数就是负的对数似然，它与训练数据和模型分布间的交叉熵等价**\n",
    "   \n",
    "   8.贯穿神经网络设计的一个反复出现的主题是代价函数的梯度必须必须足够大和足有足够的预测性，来为学习提供一个好的指引。饱和的函数破坏了这一目标，因为它们把梯度变得非常小。\n",
    "   \n",
    "   9.在很多情况下，很多输出单元都会包含一个指数函数，这在它的变量绝对值很大时会造成饱和。负的对数似然可以在很多模型中避免这个问题。**负对数似然代价函数消除了某些输出单元中的指数效果**\n",
    "   \n",
    "   10.均方误差和平均绝对误差在使用基于梯度的优化方法时往往成效不佳。一些饱和的输出单元当结合这些代价函数时会产生非常小的梯度\n",
    "   \n",
    "### 6.2.2 输出单元\n",
    "\n",
    "   1.代价函数的选择与输出单元的选择紧密相关。大多数时候，我们简单地使用数据分布和模型分布间的交叉熵。选择如何表示输出决定了交叉熵函数的形式\n",
    "\n",
    "#### 6.2.2.1 用于高斯输出分布的线性单元\n",
    "    * 一种简单的输出单元是基于仿射变换的输出单元，仿射变换不具有非线性。这些单元往往被直接称为线性单元\n",
    "    * 线性输出层常被用来产生条件高斯分布的均值，最大化其似然函数等价于最小化均方误差\n",
    "    * 因为线性单元不会饱和，所以它们易于采用基于梯度的优化算法\n",
    "#### 6.2.2.2 用于伯努利输出分布的sigmoid单元\n",
    "    * z = W*x+b\n",
    "    * y = σ(z)=W*v+b\n",
    "    * 许多任务需要预测二值型变量y的值，具有两个类的分类问题可以归结为这种形式\n",
    "    * 此时最大似然方法是定义y在x条件下的伯努利分布\n",
    "    * 伯努利分布仅需单个参数来定义。神经网络只需要预测P(y=1|x)即可。为了让这个数是有效的概率，它必须处在区间(0,1)中\n",
    "    * 我们希望找到一种方法来保证，无论何时模型给出错误答案时总能有一个较大的梯度\n",
    "    * 这种方法是基于使用sigmoid输出单元结合最大似然来实现的\n",
    "    * 我们可以认为sigmoid输出单元具有两个部分，首先，它使用一个线性层进行计算，其次，使用sigmoid激活函数将线性计算的结果转换成非归一化(和不为1)概率\n",
    "    * sigmoid可以通过先构造一个非归一化的概率分布来得到，随后除以一个合适的常数来得到有效的概率分布。\n",
    "    * 基于指数和归一化的概率分布在统计建模中很常用。用于定义这种二值型变量分布变量的变量被称为分对数(logit)\n",
    "    * 这种在对数空间里预测概率的方法可以很自然地使用最大似然学习。因为用于最大似然的代价函数是-logP(y|x),代价函数中的log抵消了sigmoid中的exp。如果使用其他损失函数，sigmoid的饱和性会阻止基于梯度学习做出好的改进\n",
    "    * 负的最大似然几乎总是训练sigmoid输出单元的优选方法\n",
    "#### 6.2.2.3 用于多项输出分布softmax单元\n",
    "    * 当想要表示一个具有n个可能取值的离散型随机变量的分布时，都可以使用softmax函数，它可以看作sigmoid函数的扩展，其中sigmoid函数用来表示二值型变量的分布\n",
    "    * softmax的名称可能会让人产生疑惑。这个函数更接近于argmax函数而不是max函数\n",
    "    * softmax函数是对输入之间差异的响应\n",
    "    * softmax函数最常用作分类器的输出，来表示n个不同类上的概率分布\n",
    "    * 将softmax定义成指数的形式是很自然的，因为对数似然中的log可以抵消softmax中的exp\n",
    "    * 与sigmoid类似，softmax函数也会饱和。对于softmax的情况，它有多个输出值，当输入值之间的差异非常极端时，这些输出值可能饱和。当softmax饱和时，基于softmax的许多代价函数也饱和，除非它们能够转化饱和的激活函数\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
